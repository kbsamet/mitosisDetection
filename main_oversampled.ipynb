{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        y_true = K.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true * alpha + (K.ones_like(y_true) - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (K.ones_like(y_true) - y_true) * (1 - y_pred)\n",
    "        fl = - alpha_t * K.pow((K.ones_like(y_true) - p_t), gamma) * K.log(p_t)\n",
    "        return K.mean(fl)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def load_images_and_masks(data_dir):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for patient_folder in os.listdir(data_dir):\n",
    "        if not os.path.isdir(os.path.join(data_dir, patient_folder)):\n",
    "            continue\n",
    "        patient_path = os.path.join(data_dir, patient_folder)\n",
    "        \n",
    "        for file in os.listdir(patient_path):\n",
    "            if file.endswith('.bmp'):\n",
    "                # Load image\n",
    "               \n",
    "                image_path = os.path.join(patient_path, file)\n",
    "                image = cv2.imread(image_path)\n",
    "                images.append(image)\n",
    "                \n",
    "                # Create corresponding mask\n",
    "                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "                csv_file = file.replace('.bmp', '.csv')\n",
    "                csv_path = os.path.join(patient_path, csv_file)\n",
    "                \n",
    "                if os.path.exists(csv_path):\n",
    "                    try:\n",
    "                        with open(csv_path, 'r') as f:\n",
    "                            lines = f.readlines()\n",
    "                            for line in lines:\n",
    "                                values = line.strip().split(',')\n",
    "                                # Ensure even number of values (pairs of coordinates)\n",
    "                                if len(values) % 2 != 0:\n",
    "                                    print(f\"Warning: Skipping invalid row in {csv_path}: {line}\")\n",
    "                                    continue\n",
    "                                # Convert pairs of coordinates to integers and set mask\n",
    "                                for i in range(0, len(values), 2):\n",
    "                                    try:\n",
    "                                        col_idx = int(values[i])\n",
    "                                        row_idx = int(values[i+1])\n",
    "                                        mask[row_idx, col_idx] = 1\n",
    "                                    except ValueError:\n",
    "                                        print(f\"Warning: Skipping invalid coordinate pair in {csv_path}: ({values[i]}, {values[i+1]})\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {csv_path}: {e}\")\n",
    "                \n",
    "                masks.append(mask)\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "def extract_patches(image, patch_size=64, stride=32):\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[y:y + patch_size, x:x + patch_size]\n",
    "            patches.append(patch)\n",
    "            coordinates.append((y, x))\n",
    "    return np.array(patches), coordinates\n",
    "\n",
    "\n",
    "\n",
    "def reconstruct_mask(pred_patches, coordinates, image_shape, patch_size=64, stride=32):\n",
    "    mask = np.zeros((image_shape[0], image_shape[1]))\n",
    "    count = np.zeros((image_shape[0], image_shape[1]))\n",
    "\n",
    "    for i, (y, x) in enumerate(coordinates):\n",
    "        mask[y:y + patch_size, x:x + patch_size] += pred_patches[i].squeeze()\n",
    "        count[y:y + patch_size, x:x + patch_size] += 1\n",
    "    \n",
    "    mask = np.divide(mask, count, out=np.zeros_like(mask), where=count!=0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_augmented_generators(images, masks, batch_size=32):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        brightness_range=[0.8, 1.2]\n",
    "    )\n",
    "\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 1\n",
    "    image_datagen.fit(images, augment=True, seed=seed)\n",
    "    mask_datagen.fit(masks, augment=True, seed=seed)\n",
    "    \n",
    "    image_generator = image_datagen.flow(images, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(masks, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    return image_generator, mask_generator\n",
    "\n",
    "\n",
    "def oversample_mitosis_patches_generator(images, masks, patch_size=32, stride=16, oversample_factor=10):\n",
    "    while True:\n",
    "        for image, mask in zip(images, masks):\n",
    "            coords = np.argwhere(mask == 1)\n",
    "            for _ in range(oversample_factor):\n",
    "                for y, x in coords:\n",
    "                    y_min, y_max = max(0, y-patch_size//2), min(image.shape[0], y+patch_size//2)\n",
    "                    x_min, x_max = max(0, x-patch_size//2), min(image.shape[1], x+patch_size//2)\n",
    "                    patch = image[y_min:y_max, x_min:x_max]\n",
    "                    patch_mask = mask[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n",
    "                        yield patch, patch_mask\n",
    "\n",
    "\n",
    "def patch_generator(images, masks, patch_size=64, stride=16, batch_size=64):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "        for image, mask in zip(images, masks):\n",
    "            for y in range(0, image.shape[0] - patch_size + 1, stride):\n",
    "                for x in range(0, image.shape[1] - patch_size + 1, stride):\n",
    "                    image_patch = image[y:y + patch_size, x:x + patch_size]\n",
    "                    mask_patch = mask[y:y + patch_size, x:x + patch_size]\n",
    "                    batch_images.append(image_patch)\n",
    "                    batch_masks.append(mask_patch)\n",
    "                    if len(batch_images) == batch_size:\n",
    "                        yield np.array(batch_images), np.expand_dims(np.array(batch_masks), axis=-1)\n",
    "                        batch_images, batch_masks = [], []\n",
    "        if batch_images:\n",
    "            yield np.array(batch_images), np.expand_dims(np.array(batch_masks), axis=-1)\n",
    "\n",
    "def balanced_patch_generator(images, masks, patch_size=32, stride=16, batch_size=32, oversample_factor=10):\n",
    "    mitosis_generator = oversample_mitosis_patches_generator(images, masks, patch_size, stride, oversample_factor)\n",
    "    \n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        non_mitosis_patches = []\n",
    "        non_mitosis_coords = []\n",
    "        \n",
    "        for image, mask in zip(images, masks):\n",
    "            coords = np.argwhere(mask == 0)\n",
    "            for y, x in coords[::stride]:\n",
    "                y_min, y_max = max(0, y-patch_size//2), min(image.shape[0], y+patch_size//2)\n",
    "                x_min, x_max = max(0, x-patch_size//2), min(image.shape[1], x+patch_size//2)\n",
    "                patch = image[y_min:y_max, x_min:x_max]\n",
    "                patch_mask = mask[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n",
    "                    non_mitosis_patches.append(patch)\n",
    "                    non_mitosis_coords.append(patch_mask)\n",
    "        \n",
    "        while len(batch_images) < batch_size:\n",
    "            try:\n",
    "                if len(non_mitosis_patches) > 0:\n",
    "                    mitosis_patch, mitosis_mask = next(mitosis_generator)\n",
    "                    batch_images.append(mitosis_patch)\n",
    "                    batch_masks.append(mitosis_mask)\n",
    "                    batch_images.append(non_mitosis_patches.pop())\n",
    "                    batch_masks.append(non_mitosis_coords.pop())\n",
    "                else:\n",
    "                    break\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        if len(batch_images) > 0:\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_masks = np.expand_dims(np.array(batch_masks), axis=-1)\n",
    "\n",
    "            # Apply augmentations\n",
    "            image_generator, mask_generator = create_augmented_generators(batch_images, batch_masks, batch_size=batch_size)\n",
    "            augmented_images = next(image_generator)\n",
    "            augmented_masks = next(mask_generator)\n",
    "            \n",
    "            yield augmented_images, augmented_masks\n",
    "\n",
    "\n",
    "def generator_to_dataset(generator, output_shape, batch_size):\n",
    "    def gen():\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "            \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(batch_size, *output_shape), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(batch_size, *output_shape, 1), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def cross_validation(images, masks, n_splits=5, patch_size=32, stride=16, batch_size=32, oversample_factor=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for train_index, val_index in kf.split(images):\n",
    "        X_train, X_val = images[train_index], images[val_index]\n",
    "        y_train, y_val = masks[train_index], masks[val_index]\n",
    "\n",
    "        train_generator = balanced_patch_generator(X_train, y_train, patch_size, stride, batch_size, oversample_factor)\n",
    "        val_generator = balanced_patch_generator(X_val, y_val, patch_size, stride, batch_size, oversample_factor)\n",
    "        \n",
    "        num_train_patches = sum(\n",
    "            (image.shape[0] - patch_size + 1) * (image.shape[1] - patch_size + 1) // (stride * stride)\n",
    "            for image in X_train\n",
    "        )\n",
    "        num_val_patches = sum(\n",
    "            (image.shape[0] - patch_size + 1) * (image.shape[1] - patch_size + 1) // (stride * stride)\n",
    "            for image in X_val\n",
    "        )\n",
    "\n",
    "        fold_results.append((train_generator, val_generator, num_train_patches // batch_size, num_val_patches // batch_size, X_val, y_val))\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "def unet_model(input_size=(64, 64, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    up6 = Concatenate()([up6, conv4])\n",
    "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Concatenate()([up7, conv3])\n",
    "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up8 = Concatenate()([up8, conv2])\n",
    "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up9 = Concatenate()([up9, conv1])\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Flatten the arrays to calculate the metrics at the pixel level\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Check for shape consistency\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(f\"Inconsistent number of samples: {len(y_true)} != {len(y_pred)}\")\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35 images and 35 masks.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'scanner_A'  # Your data directory\n",
    "input_size = (64, 64, 3)  # Define the input size for the model\n",
    "\n",
    "# Load images and masks\n",
    "images, masks = load_images_and_masks(data_dir)\n",
    "print(f\"Loaded {len(images)} images and {len(masks)} masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "Mitosis pixels: 135376, Non-mitosis pixels: 151871584.0\n"
     ]
    }
   ],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "# Check class distribution\n",
    "total_pixels = np.prod(masks.shape)\n",
    "num_mitosis_pixels = np.sum(masks)\n",
    "num_non_mitosis_pixels = total_pixels - num_mitosis_pixels\n",
    "print(f\"Mitosis pixels: {num_mitosis_pixels}, Non-mitosis pixels: {num_non_mitosis_pixels}\")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "fold_results = cross_validation(images[:2], masks , n_splits=2,patch_size=64, stride=32, batch_size=256,oversample_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 8s/step - Precision: 0.0328 - Recall: 0.2866 - accuracy: 0.7157 - loss: 2.7568 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.9743 - val_loss: 0.0110\n",
      "Model trained on fold 1\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 212ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samet.kucukbayraktar/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "Fold 2\n",
      "\u001b[1m 2/15\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 6s/step - Precision: 0.0357 - Recall: 0.0732 - accuracy: 0.9224 - loss: 0.2361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m unet_model(input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model using the training generator and validate it on the validation data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust the number of epochs as needed\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained on fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold_num, (train_gen, val_gen, train_steps, val_steps, X_val, y_val) in enumerate(fold_results):\n",
    "    print(f\"Fold {fold_num + 1}\")\n",
    "\n",
    "    # Define and compile the model\n",
    "    model = unet_model(input_size=input_size)\n",
    "    # Train the model using the training generator and validate it on the validation data\n",
    "    model.fit(train_gen,\n",
    "                validation_data=val_gen,\n",
    "                epochs=1,  # Adjust the number of epochs as needed\n",
    "                steps_per_epoch=train_steps,\n",
    "                validation_steps=val_steps, \n",
    "                verbose=1)\n",
    "    \n",
    "    print(f\"Model trained on fold {fold_num + 1}\")\n",
    "    model.save('model.keras')\n",
    "\n",
    "    # Reconstruct the full validation mask\n",
    "    patches, coords = extract_patches(X_val[0], patch_size=64, stride=32)\n",
    "    val_patch_preds = model.predict(patches)\n",
    "    val_patch_preds = np.nan_to_num(val_patch_preds, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    val_patch_preds = (val_patch_preds > 0.5).astype(np.uint8)  # Convert predictions to binary masks\n",
    "    val_mask_pred = reconstruct_mask(val_patch_preds, coords, X_val[0].shape, patch_size=64, stride=32)\n",
    "    \n",
    "    precision, recall, f1 = calculate_metrics(y_val.flatten(), val_mask_pred.flatten())\n",
    "    print(f\"Fold {fold_num + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = tf.keras.models.load_model('model.keras', custom_objects={'focal_loss_fixed': focal_loss()})\n",
    "for fold_num, (train_gen, val_gen, train_steps, val_steps, X_val, y_val) in enumerate(fold_results):\n",
    "    next(train_gen)\n",
    "    next(train_gen)\n",
    "    image, mask = next(train_gen)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(image[0])\n",
    "    plt.title('Image')\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(mask[0].squeeze(), cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 285ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m patch_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(patch_preds, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      9\u001b[0m patch_preds \u001b[38;5;241m=\u001b[39m (patch_preds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m---> 10\u001b[0m mask_pred \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m221\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 96\u001b[0m, in \u001b[0;36mreconstruct_mask\u001b[0;34m(pred_patches, coordinates, image_shape, patch_size, stride)\u001b[0m\n\u001b[1;32m     93\u001b[0m count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((image_shape[\u001b[38;5;241m0\u001b[39m], image_shape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (y, x) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(coordinates):\n\u001b[0;32m---> 96\u001b[0m     \u001b[43mmask\u001b[49m[y:y \u001b[38;5;241m+\u001b[39m patch_size, x:x \u001b[38;5;241m+\u001b[39m patch_size] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred_patches[i]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     97\u001b[0m     count[y:y \u001b[38;5;241m+\u001b[39m patch_size, x:x \u001b[38;5;241m+\u001b[39m patch_size] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     99\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m/\u001b[39m count\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dev/mitosisDetection/venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.keras', custom_objects={'focal_loss_fixed': focal_loss()})\n",
    "image = images[0]\n",
    "mask = masks[0].squeeze()\n",
    "patch_size = 64\n",
    "stride = 32\n",
    "patches, coords = extract_patches(image, patch_size=patch_size, stride=stride)\n",
    "patch_preds = model.predict(patches)\n",
    "patch_preds = np.nan_to_num(patch_preds, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "patch_preds = (patch_preds > 0.5).astype(np.uint8)\n",
    "mask_pred = reconstruct_mask(patch_preds, coords, image.shape, patch_size=patch_size, stride=stride)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(image)\n",
    "plt.title('Image')\n",
    "plt.subplot(222)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.subplot(223)\n",
    "plt.imshow(mask_pred, cmap='gray')\n",
    "plt.title('Predicted Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training and evaluation completed.\")\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
